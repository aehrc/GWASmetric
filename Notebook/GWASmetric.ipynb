{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GWASmetric\n",
    "There are number of ways to measure association power of a SNP with respect to a binary phenotype. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialise Hail v0.2\n",
    "Try using [VariantSpark on the AWS MarketPlace](https://aws.amazon.com/marketplace/pp/AEHRC-VariantSpark-Notebook/B07YVND4TD) which provide you a Jupyter notebook with VariantSpark and Hail v0.2 installed and ready to use.\n",
    "\n",
    "See the step-by-step instruction [here](https://drive.google.com/file/d/1pD40H1wseP8RhrV6XnkkyvZMqxqkN_N5/view?usp=sharing)\n",
    "\n",
    "Hail is configured to read/write from/to AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "import hail as hl\n",
    "import varspark.hail as vshl\n",
    "vshl.init(sc=sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Plink\n",
    "If you dont have a plink on your machine uncomment below cell to downlaod it (no installation is requiered)\n",
    "\n",
    "This cell download plink for a 64bit linux operating system and extract it in the current directory.\n",
    "\n",
    "Change plink download path if you use a different operating system.  \n",
    "\n",
    "In case you already have plink on your machine replace \"./plink\" with \"plink\" in all other cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%sh\n",
    "# wget http://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20200616.zip &> temp\n",
    "# unzip plink_linux_x86_64_20200616.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulation Parameter\n",
    "in our setup, Hail read/write data from/to AWS S3 storage. If this is not yor setup you should manually change part of this notebook. The easeiest way is to use [VariantSpark on the AWS MarketPlace](https://aws.amazon.com/marketplace/pp/AEHRC-VariantSpark-Notebook/B07YVND4TD) described above.\n",
    "\n",
    "Note that this notebook also use the local storage and write some files.\n",
    "\n",
    "| step | noHom=True | noHom=False |\n",
    "|:----:|:----------:|:-----------:|\n",
    "|  0.5 |          9 |          36 |\n",
    "|  0.1 |        121 |        4356 |\n",
    "| 0.05 |        441 |       53361 |\n",
    "| 0.01 |      10201 |    26532801 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCase=5000 # total number of cases\n",
    "numCtrl=5000 # total number of controls\n",
    "step=0.1     # Granuality of the simulation, more variants are simulated with lower step (0<step<1)\n",
    "             # For the number of simulated variants see above table\n",
    "\n",
    "# S3 path where all files are stored.\n",
    "s3=\"s3://your-bucket/your-path/\"\n",
    "\n",
    "# Output file prefix\n",
    "ofn=\"output\"\n",
    "\n",
    "# noHom=True keep the number of Homozygeour Alternate Genotype (1/1) 0.\n",
    "# So that each SNP can take only 2 values 0/0 or 0/1\n",
    "# This is simpler case where you can decrease the step to a smaller value\n",
    "noHom=False \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init Bash command\n",
    "Write the ofn and s3 parameters to the file to be read by bash (%%sh) cells.\n",
    "\n",
    "Why we don't use python subprocess? That's a good question! :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ofn', 'w') as file:\n",
    "    file.write(ofn)\n",
    "with open('s3', 'w') as file:\n",
    "    file.write(s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Purity\n",
    "These functions compute the set purity using Gini and Entropy. Set purity is simply 1-(set impurity).\n",
    "\n",
    "Asume A$$ and B represent the number of cases and controls in a set where T=A+B is the number of all samples in the set. Then PA and PB represent the probability of cases and controls.\n",
    "\n",
    "$$P_A=\\frac{A}{T}$$\n",
    "\n",
    "$$P_B=\\frac{B}{T}$$\n",
    "\n",
    "$$GiniPurity={P_A}^2+{P_B}^2$$\n",
    "\n",
    "$$EntropyPurity=1-(P_A\\times \\log_2(P_A) + P_B\\times \\log_2(P_B))$$\n",
    "\n",
    "While entropy range between 0 and 1, Gini range from 0.5 to 1. Note that if there are more than two clasess (case, control) the mimumum Gini would be smaller. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GiniPurity(A, B):\n",
    "    T=A+B\n",
    "    GP = 0\n",
    "    if T:\n",
    "        PA = A/T\n",
    "        PB = B/T\n",
    "        GP = (PA**2)+(PB**2)\n",
    "    return GP\n",
    "\n",
    "def EntropyPurity(A, B):\n",
    "    T=A+B\n",
    "    EP = 0\n",
    "    if T:\n",
    "        PA = A/T\n",
    "        PB = B/T\n",
    "        EP = 1\n",
    "        EP += (PA*np.log2(PA)) if PA else 0\n",
    "        EP += (PB*np.log2(PB)) if PB else 0\n",
    "    return EP\n",
    "\n",
    "#### Test Code\n",
    "# A=10\n",
    "# for B in range(0,11):\n",
    "#     print(GiniPurity(A,B), EntropyPurity(A,B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Gained (IG)\n",
    "When a variable splits a set into multiple subsets, the information gained is the difference between weither average purity of subsets and the purity of the parent set. In the average, the weight is the fraction of samples in the subset compared to the parent set.\n",
    "\n",
    "In case of a SNP, the parent set is all of the samples (all cases and all controls) which is divided into three groups base on their genotype for the given SNP (0/0, 0/1, 1/1) or (Ref, Het, Hom)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IG: Information Gained\n",
    "def GiniIG(ParentSetPurity, CaseRef, CaseHet, CaseHom, CtrlRef, CtrlHet, CtrlHom):\n",
    "    total = float(CaseRef + CaseHet + CaseHom + CtrlRef + CtrlHet + CtrlHom)\n",
    "    InformationGained = 0\n",
    "    if total:\n",
    "        ######         Set Purity                  Weight\n",
    "        Ref = GiniPurity(CaseRef, CtrlRef) * ((CaseRef + CtrlRef) / total)\n",
    "        Het = GiniPurity(CaseHet, CtrlHet) * ((CaseHet + CtrlHet) / total)\n",
    "        Hom = GiniPurity(CaseHom, CtrlHom) * ((CaseHom + CtrlHom) / total)\n",
    "        WeightedAverageSetPurity = Ref + Het + Hom\n",
    "        InformationGained = WeightedAverageSetPurity - ParentSetPurity\n",
    "    return InformationGained\n",
    "\n",
    "\n",
    "def EntropyIG(ParentSetPurity, CaseRef, CaseHet, CaseHom, CtrlRef, CtrlHet, CtrlHom):\n",
    "    total = float(CaseRef + CaseHet + CaseHom + CtrlRef + CtrlHet + CtrlHom)\n",
    "    InformationGained = 0\n",
    "    if total:\n",
    "        ######         Set Purity                  Weight\n",
    "        Ref = EntropyPurity(CaseRef, CtrlRef) * ((CaseRef + CtrlRef) / total)\n",
    "        Het = EntropyPurity(CaseHet, CtrlHet) * ((CaseHet + CtrlHet) / total)\n",
    "        Hom = EntropyPurity(CaseHom, CtrlHom) * ((CaseHom + CtrlHom) / total)\n",
    "        WeightedAverageSetPurity = Ref + Het + Hom\n",
    "        InformationGained = WeightedAverageSetPurity - ParentSetPurity\n",
    "    return InformationGained\n",
    "\n",
    "### Simple test for information gaine function\n",
    "# print(GiniIG   (0.5, 3, 5, 2, 7, 2, 1))\n",
    "# print(EntropyIG(0.0, 3, 5, 2, 7, 2, 1))\n",
    "# print(\"==============================\")\n",
    "# print(GiniIG   (0.5, 8, 1, 1, 7, 2, 1))\n",
    "# print(EntropyIG(0.0, 8, 1, 1, 7, 2, 1))\n",
    "# print(\"==============================\")\n",
    "# print(GiniIG   (0.5, 1, 1, 8, 7, 2, 1))\n",
    "# print(EntropyIG(0.0, 1, 1, 8, 7, 2, 1))\n",
    "# print(\"==============================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNP simulation\n",
    "## Also compute Gini and Entropy Information Gained for SNPs\n",
    "All different possibility of the contingency table (given number of cases and controls) are considered and the corresponding SNP genotype is produced for population of cases and controls.\n",
    "\n",
    "The contingency table is described below and the nested loop in the function generate all possible value with granuality of the step.\n",
    "\n",
    "|  Genotype |   Case  |   Ctrl  |\n",
    "|:---------:|:-------:|:-------:|\n",
    "| 0/0 (Ref) | refCase | refCtrl |\n",
    "| 0/1 (Het) | hetCase | hetCtrl |\n",
    "| 1/1 (Hom) | homCase | homCtrl |\n",
    "\n",
    "SimulateSNPs_noHom consider situation where homCase and homCtrl are always 0.\n",
    "\n",
    "For the below contingency table, it does not matter the genotype are produced in formA, formB, or any other form, the association power should remains the same (i.e. the order of sample in the input data does not affect association power). Note that hear we consider individual SNP association power and does not consider any covariate.\n",
    "\n",
    "|  Genotype | Case | Ctrl |\n",
    "|:---------:|:----:|:----:|\n",
    "| 0/0 (Ref) |   3  |   1  |\n",
    "| 0/1 (Het) |   0  |   2  |\n",
    "| 1/1 (Hom) |   1  |   1  |\n",
    "\n",
    " | Form | case1 | case2 | case3 | case4 | ctrl1 | ctrl2 | ctrl3 | ctrl4 |\n",
    "|------|-------|-------|-------|-------|-------|-------|-------|-------|\n",
    "| A    | 0/0   | 0/0   | 0/0   | 1/1   | 0/0   | 0/1   | 0/1   | 1/1   |\n",
    "| B    | 0/0   | 0/1   | 0/0   | 0/0   | 0/1   | 1/1   | 0/0   | 0/1   |\n",
    "\n",
    "**Hardy-Weinberg is not considered in this simulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimulateSNPs(writer, giniParentSetPurity, entropyParentSetPurity, numCase, numCtrl, step):\n",
    "    \n",
    "    stepCtrl = int(numCase * step)\n",
    "    stepCase = int(numCtrl * step)\n",
    "    \n",
    "    \n",
    "    pos=0\n",
    "    for refCase in range(0, numCase+1, stepCase):  \n",
    "        for hetCase in range(0, (numCase-refCase)+1, stepCase):\n",
    "            homCase = numCase - (refCase + hetCase)\n",
    "            for refCtrl in range(0, numCtrl+1, stepCtrl):\n",
    "                for hetCtrl in range(0, (numCtrl-refCtrl)+1, stepCtrl):\n",
    "                    homCtrl = numCtrl - (refCtrl + hetCtrl)\n",
    "                    \n",
    "                    pos+=1\n",
    "                    \n",
    "                    giniIG    = GiniIG   (giniParentSetPurity   , refCase, hetCase, homCase, refCtrl, hetCtrl, homCtrl)\n",
    "                    entropyIG = EntropyIG(entropyParentSetPurity, refCase, hetCase, homCase, refCtrl, hetCtrl, homCtrl)\n",
    "                    \n",
    "                    snpID  =     str(refCase)\n",
    "                    snpID += \"#\"+str(hetCase)\n",
    "                    snpID += \"#\"+str(homCase)\n",
    "                    snpID += \"#\"+str(refCtrl)\n",
    "                    snpID += \"#\"+str(hetCtrl)\n",
    "                    snpID += \"#\"+str(homCtrl)\n",
    "                    snpID += \"#\"+str(giniIG)\n",
    "                    snpID += \"#\"+str(entropyIG)\n",
    "                    \n",
    "                    genotype = [\"1\", pos, snpID, \"A\", \"C\", \".\", \".\", \".\", \"GT\"]\n",
    "                    genotype += [\"0/0\"]*refCtrl\n",
    "                    genotype += [\"0/1\"]*hetCtrl\n",
    "                    genotype += [\"1/1\"]*homCtrl\n",
    "                    genotype += [\"0/0\"]*refCase\n",
    "                    genotype += [\"0/1\"]*hetCase\n",
    "                    genotype += [\"1/1\"]*homCase\n",
    "                    writer.writerow(genotype)\n",
    "    print(\"SNPs simulated: \", pos)\n",
    "\n",
    "def SimulateSNPs_noHom(writer, giniParentSetPurity, entropyParentSetPurity, numCase, numCtrl, step):\n",
    "    \n",
    "    stepCtrl = int(numCase * step)\n",
    "    stepCase = int(numCtrl * step)\n",
    "    \n",
    "    \n",
    "    pos=0\n",
    "    for refCase in range(0, numCase+1, stepCase):  \n",
    "        hetCase = numCase-refCase\n",
    "        homCase = 0\n",
    "        for refCtrl in range(0, numCtrl+1, stepCtrl):\n",
    "            hetCtrl = numCtrl-refCtrl\n",
    "            homCtrl = 0\n",
    "\n",
    "            pos+=1\n",
    "\n",
    "            giniIG    = GiniIG   (giniParentSetPurity   , refCase, hetCase, homCase, refCtrl, hetCtrl, homCtrl)\n",
    "            entropyIG = EntropyIG(entropyParentSetPurity, refCase, hetCase, homCase, refCtrl, hetCtrl, homCtrl)\n",
    "\n",
    "            snpID  =     str(refCase)\n",
    "            snpID += \"#\"+str(hetCase)\n",
    "            snpID += \"#\"+str(homCase)\n",
    "            snpID += \"#\"+str(refCtrl)\n",
    "            snpID += \"#\"+str(hetCtrl)\n",
    "            snpID += \"#\"+str(homCtrl)\n",
    "            snpID += \"#\"+str(giniIG)\n",
    "            snpID += \"#\"+str(entropyIG)\n",
    "\n",
    "            genotype = [\"1\", pos, snpID, \"A\", \"C\", \".\", \".\", \".\", \"GT\"]\n",
    "            genotype += [\"0/0\"]*refCtrl\n",
    "            genotype += [\"0/1\"]*hetCtrl\n",
    "            genotype += [\"1/1\"]*homCtrl\n",
    "            genotype += [\"0/0\"]*refCase\n",
    "            genotype += [\"0/1\"]*hetCase\n",
    "            genotype += [\"1/1\"]*homCase\n",
    "            writer.writerow(genotype)\n",
    "    print(\"SNPs simulated: \", pos)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VCF File Simulation\n",
    "To compute association power using plink, Hail and other GWAS software\n",
    "we cannot directly input the contingency table.\n",
    "Thus we generate VCF file where the SNP genotype reflect the contingency table that we produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simulate(ofn=\"test\", numCase=10, numCtrl=10, step=0.1):\n",
    "    \n",
    "    # Compute Parent set purity\n",
    "    giniParentSetPurity    = GiniPurity(numCase, numCtrl)\n",
    "    entropyParentSetPurity = EntropyPurity(numCase, numCtrl)\n",
    "    print(\"Gini    Parent Set Purity\", giniParentSetPurity)\n",
    "    print(\"Entropy Parent Set Purity\", entropyParentSetPurity)\n",
    "\n",
    "    # Simulte VCF file\n",
    "    with open(ofn+'.vcf','w') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        writer.writerow([\"##fileformat=VCFv4.2\"])\n",
    "        writer.writerow(['##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">'])\n",
    "        header=[\"#CHROM\",\"POS\",\"ID\",\"REF\",\"ALT\",\"QUAL\",\"FILTER\",\"INFO\",\"FORMAT\"]\n",
    "        for ctrl in range(1,numCtrl+1):\n",
    "            header.append(\"ctrl\"+str(ctrl))\n",
    "        for case in range(1,numCase+1):\n",
    "            header.append(\"case\"+str(case))\n",
    "        writer.writerow(header)\n",
    "\n",
    "        if noHom:\n",
    "            SimulateSNPs_noHom(writer, giniParentSetPurity, entropyParentSetPurity, numCase, numCtrl, step)\n",
    "        else:\n",
    "            SimulateSNPs(writer, giniParentSetPurity, entropyParentSetPurity, numCase, numCtrl, step)\n",
    "    \n",
    "    # Simulate phenotype file compatible with plink --pheno\n",
    "    with open(ofn+'.pheno','w') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter='\\t', quotechar=' ', quoting=csv.QUOTE_MINIMAL)\n",
    "        for ctrl in range(1,numCtrl+1):\n",
    "            line = [\"ctrl\"+str(ctrl), \"ctrl\"+str(ctrl), 1]\n",
    "            writer.writerow(line)\n",
    "        for case in range(1,numCase+1):\n",
    "            line = [\"case\"+str(case), \"case\"+str(case), 2]\n",
    "            writer.writerow(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Simulate(ofn=ofn, numCase=numCase, numCtrl=numCtrl, step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plink\n",
    "We use plink to do the following:\n",
    "- Convert VCF-Pheno to Plink bfile\n",
    "- Compute Plink assocciation power (There are multiple options we only use the basic one)\n",
    "- upload plink and result into aws s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ofn=$(cat ofn)\n",
    "s3=$(cat s3)\n",
    "echo $ofn\n",
    "echo $s3\n",
    "\n",
    "# Convert VCF to plink\n",
    "./plink --vcf $ofn.vcf --pheno $ofn.pheno --make-bed --out $ofn --allow-no-sex &> temp\n",
    "aws s3 cp $ofn.bed $s3\n",
    "aws s3 cp $ofn.bim $s3\n",
    "aws s3 cp $ofn.fam $s3\n",
    "\n",
    "# Perform assoc and hardy analysis\n",
    "./plink --bfile $ofn --assoc --out $ofn --allow-no-sex &> temp\n",
    "#./plink --bfile $ofn --hardy --out $ofn --allow-no-sex &> temp\n",
    "\n",
    "# Extract informaiton\n",
    "tail -n +2 $ofn.assoc | awk '{OFS=\"\\t\";print($2,$8,$9)}' | awk 'BEGIN{print(\"rsid\\tplink_chi2\\tplink_pval\")}{print}' > $ofn.GiniPval.tsv\n",
    "aws s3 cp $ofn.GiniPval.tsv $s3\n",
    "# tail -n +2 $ofn.assoc | awk '{OFS=\"\\t\";print($2,$9)}' | tr '#' \\\\t | awk 'BEGIN{print(\"refCase\\thetCase\\thomCase\\trefCtrl\\thetCtrl\\thomCtrl\\tGiniIG\\tEntropyIG\\tPval\")}{print}' > $ofn.GiniPval.tsv\n",
    "\n",
    "#head $ofn.GiniPval.tsv\n",
    "#head $ofn.frq.cc\n",
    "#head $ofn.assoc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hail\n",
    "Load plink file from AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfile=s3+ofn\n",
    "mt = hl.import_plink(bed=bfile+'.bed', bim=bfile+'.bim',fam=bfile+'.fam').repartition(32).cache()\n",
    "print(mt.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hail: Perform 3 different logistic regression test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = hl.logistic_regression_rows(test='score', y=mt.is_case, x=mt.GT.n_alt_alleles(), covariates=[1.0], pass_through=[mt.rsid])\\\n",
    ".key_by('rsid').rename({'p_value' : 'score_pval', 'chi_sq_stat' : 'score_chi2'})\n",
    "\n",
    "lrt   = hl.logistic_regression_rows(test='lrt'  , y=mt.is_case, x=mt.GT.n_alt_alleles(), covariates=[1.0], pass_through=[mt.rsid])\\\n",
    ".key_by('rsid').rename({'p_value' : 'lrt_pval', 'chi_sq_stat' : 'lrt_chi2'})\n",
    "\n",
    "wald  = hl.logistic_regression_rows(test='wald' , y=mt.is_case, x=mt.GT.n_alt_alleles(), covariates=[1.0], pass_through=[mt.rsid])\\\n",
    ".key_by('rsid').rename({'p_value' : 'wald_pval', 'z_stat' : 'wald_zstat'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Plink assoc data and join with Hail association results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = score.select(score.score_pval,score.score_chi2)\n",
    "lrt = lrt.select(lrt.lrt_pval,lrt.lrt_chi2)\n",
    "wald = wald.select(wald.wald_pval,wald.wald_zstat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plink = hl.import_table(bfile+'.GiniPval.tsv').key_by('rsid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simulated rsid is the combination of these refCase#hetCase#homCase#refCtrl#hetCtrl#homCtrl#GiniIG#EntropyIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = score.join(lrt).join(wald).join(plink)\n",
    "lr.describe()\n",
    "lr = lr.rename({'rsid' : 'refCase#hetCase#homCase#refCtrl#hetCtrl#homCtrl#GiniIG#EntropyIG'})\n",
    "lr.export(bfile+'.all.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uplaod to S3\n",
    "Break rsid to multiple feilds and upload result into S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "ofn=$(cat ofn)\n",
    "s3=$(cat s3)\n",
    "aws s3 cp $s3$ofn.all.tsv ./\n",
    "cat $ofn.all.tsv | tr '#' \\\\t > $ofn.fix.tsv\n",
    "aws s3 cp $ofn.fix.tsv $s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for Plots\n",
    "Read the data and compute -log10 of all pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pd.read_csv(ofn+'.fix.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = pdf.replace(0, np.nan)\n",
    "\n",
    "pdf['log10_score_pval'] = -np.log10(pdf['score_pval'])\n",
    "pdf['log10_lrt_pval'] = -np.log10(pdf['lrt_pval'])\n",
    "pdf['log10_wald_pval'] = -np.log10(pdf['wald_pval'])\n",
    "pdf['log10_plink_pval'] = -np.log10(pdf['plink_pval'])\n",
    "\n",
    "pdf = pdf.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pvalue vs Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.plot(x='log10_wald_pval', y='GiniIG', style='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.plot(x='log10_plink_pval', y='GiniIG', style='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pvalue vs Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.plot(x='log10_wald_pval', y='EntropyIG', style='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdf.plot(x='log10_plink_pval', y='EntropyIG', style='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gini vs Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.plot(x='EntropyIG', y='GiniIG', style='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pvalue vs Pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.plot(x='log10_plink_pval', y='log10_wald_pval', style='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.plot(x='log10_score_pval', y='log10_wald_pval', style='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.plot(x='log10_score_pval', y='log10_lrt_pval', style='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi2 vs Chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.plot(x='plink_chi2', y='score_chi2', style='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.plot(x='lrt_chi2', y='score_chi2', style='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.plot(x='plink_chi2', y='wald_zstat', style='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi2 vs Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.plot(x='wald_zstat', y='GiniIG', style='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf.plot(x='plink_chi2', y='GiniIG', style='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi2 vs Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdf.plot(x='wald_zstat', y='EntropyIG', style='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdf.plot(x='plink_chi2', y='EntropyIG', style='o')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
